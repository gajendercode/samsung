{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset for diffusion model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\aviator\\Music\\SIDD_Small_sRGB_Only\\noisy_imgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(path):\n",
    "    # Load the larger image\n",
    "    \n",
    "    larger_image_path = path\n",
    "    larger_image = Image.open(larger_image_path)\n",
    "\n",
    "    # Convert the PIL image to a PyTorch tensor\n",
    "    transform = transforms.ToTensor()\n",
    "    larger_image_tensor = transform(larger_image)\n",
    "\n",
    "    # Define the crop size (height and width)\n",
    "    crop_height = 128\n",
    "    crop_width = 128\n",
    "\n",
    "    # Calculate the number of crops in the height and width directions\n",
    "    num_crops_height = larger_image_tensor.shape[1] // crop_height\n",
    "    num_crops_width = larger_image_tensor.shape[2] // crop_width\n",
    "\n",
    "    # Initialize a list to store the cropped images\n",
    "    cropped_images = []\n",
    "\n",
    "    # Iterate through each crop position\n",
    "    for i in range(num_crops_height):\n",
    "        for j in range(num_crops_width):\n",
    "            # Calculate the starting and ending indices for cropping\n",
    "            start_height = i * crop_height\n",
    "            end_height = start_height + crop_height\n",
    "            start_width = j * crop_width\n",
    "            end_width = start_width + crop_width\n",
    "\n",
    "            # Crop the image\n",
    "            cropped_image = larger_image_tensor[:, start_height:end_height, start_width:end_width]\n",
    "\n",
    "            # Append the cropped image to the list\n",
    "            cropped_images.append(cropped_image)\n",
    "    return cropped_images\n",
    "    # The 'cropped_images' list now contains all the cropped images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img=[]\n",
    "for i,image in enumerate(os.listdir()):\n",
    "    cropped_images=get_images(f\"{image}\")\n",
    "    all_img.append(cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
